{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "333368e3-bb42-466c-8823-c45ed71f970f",
   "metadata": {},
   "source": [
    "# Prospecção de Dados (Data Mining) DI/FCUL - HA3\n",
    "\n",
    "## Third Home Assignement (MC/DI/FCUL - 2024)\n",
    "\n",
    "### Fill in the section below\n",
    "\n",
    "### GROUP:`02`\n",
    "\n",
    "* João Martins, 62532 - Hours worked on the project: 8\n",
    "* Rúben Torres, 62531 - Hours worked on the project: 8\n",
    "* Nuno Pereira, 56933 - Hours worked on the project: 8\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "The purpose of this Home Assignment is\n",
    "* Find similar items with Local Sensitivity Hashing\n",
    "* Do Dimensionality Reduction\n",
    "\n",
    "**NOTE 1: Students are not allowed to add more cells to the notebook**\n",
    "\n",
    "**NOTE 2: The notebook must be submited fully executed**\n",
    "\n",
    "**NOTE 3: Name of notebook should be: HA3_GROUP-XX.ipynb (where XX is the group number)**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a40f24-0da5-4b96-90f3-84259cbd1bc5",
   "metadata": {},
   "source": [
    "## 1. Read the Dataset\n",
    "\n",
    "The dataset correspond to about 99% of the Human Proteome (set of known Human Proteins - about 19,500), coded with specific structural elements. They are presented in a dictionary where the key is the [UniprotID](https://www.uniprot.org/) of the protein and the value is a set of indices of a specific structural characteristic\n",
    "\n",
    "Students can use one of two datasets, that are **not** subsets of each other: \n",
    "* `data_d3.pickle` - smaller set of structural features (2048)\n",
    "* `data_d4.pickle` - much larger set of structural features (20736) **Note:** This dataset has been Zipped to fit into moodle. Students should unzip it before usage \n",
    "\n",
    "Select **one** of the datasets and perform all analyses with it. \n",
    "\n",
    "It may be adviseable the usage of sparse matrices, especially for the `d4` dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "aa918e91-c325-4c1d-8e8d-93d411035a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your code Here\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "#'A0A024R1R8', 'A0A024RBG1', 'A0A075B6H7', 'A0A075B6H8', 'A0A075B6H9', 'A0A075B6I0', 'A0A075B6I1'\n",
    "\n",
    "# human_proteins_dataset= pd.read_pickle(\"data_d3.pickle\")\n",
    "human_proteins_dataset =pickle.load(open(\"data_d3.pickle\", \"rb\"))\n",
    "# dataframe = pd.DataFrame(list(human_proteins_dataset.items()), columns=[\"Id\", \"Proteome\"])\n",
    "# dataframe[\"Proteome\"]\n",
    "\n",
    "# for protein in dataframe[\"Proteome\"][:20]:\n",
    "#     print(protein)\n",
    "#     print(len(protein))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853614f3-1e1b-4481-9784-e29dd0b453cb",
   "metadata": {},
   "source": [
    "## 2. Perform Local Sensitivity Hashing (LSH)\n",
    "\n",
    "* examine the selected dataset in terms of similarities and select a set of LSH parameters able to capture the most similar proteins\n",
    "* Comment your results\n",
    "\n",
    "**BONUS POINTS:** It might be interesting to identify **some** of the candidate pairs in Uniprot, to check if they share some of the same properties (e.g. for [protein P28223](https://www.uniprot.org/uniprotkb/P28223/entry))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ead806d9-8073-4fc0-9668-68ad0b89077f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MakeBucketsT(TDocs, perms, N, B, R, M, NB):\n",
    "    Buckets = {}\n",
    "    all_docs = set(range(N))\n",
    "    for b in range(B):\n",
    "        SIGS = np.zeros((N, R), dtype=\"int32\")\n",
    "        for r in range(R):\n",
    "            perm = perms[b*R + r]\n",
    "            L = all_docs.copy()\n",
    "            i = 0\n",
    "            while len(L) > 0:\n",
    "                elem = perm[i]\n",
    "                docs_found = TDocs[elem] & L\n",
    "                if len(docs_found) > 0:\n",
    "                    SIGS[list(docs_found), r] = i\n",
    "                    L = L - docs_found\n",
    "                i += 1\n",
    "                if i == M:\n",
    "                    SIGS[list(L), r] = i\n",
    "                    L = {}\n",
    "        for d in range(N):\n",
    "            bucket = hash(tuple(SIGS[d])) % NB\n",
    "            Buckets.setdefault((b, bucket), set()).add(d)\n",
    "    return Buckets\n",
    "\n",
    "def LSHTs(docs, words, B, R, NB=28934501, verbose=True):\n",
    "    N, M = len(docs), words\n",
    "    if verbose: print(\"Transpose the dataset\")\n",
    "    data = [[] for _ in range(M)]\n",
    "    for doc, i in enumerate(docs):\n",
    "        for word in i: \n",
    "            data[word].append(doc)\n",
    "    dataT = [set(L) for L in data]\n",
    "    P = B * R\n",
    "    np.random.seed(3)\n",
    "    if verbose: print(f\"Generating {P} permutations for {(1/B)**(1/R):.3f} similarity\")\n",
    "    perms = [np.random.permutation(M) for _ in range(P)]\n",
    "    if verbose: print(\"Computing buckets...\")\n",
    "    buckets = MakeBucketsT(dataT, perms, N, B, R, NB)\n",
    "    return buckets\n",
    "\n",
    "def make_word_indexes(words): \n",
    "    return dict(zip(words, range(len(words))))\n",
    "\n",
    "def make_indexed_docs(docs, word_index):\n",
    "    indexed_docs = []\n",
    "    for doc in docs:\n",
    "        indexed_doc = set(word_index[word] for word in doc)\n",
    "        indexed_docs.append(indexed_doc)\n",
    "    return indexed_docs\n",
    "\n",
    "def remove_empty_docs(words_text_sets):\n",
    "    return [doc for doc in words_text_sets if len(doc) > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ad80328c-ffba-4bb6-8ce3-03726ee82348",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transpose the dataset\n",
      "Generating 100 permutations for 0.549 similarity\n",
      "Computing buckets...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "MakeBucketsT() missing 1 required positional argument: 'NB'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m B \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m\n\u001b[0;32m      8\u001b[0m R \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m\n\u001b[1;32m----> 9\u001b[0m bucks \u001b[38;5;241m=\u001b[39m \u001b[43mLSHTs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindexed_docs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mwords\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mR\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m b, buck \u001b[38;5;129;01min\u001b[39;00m bucks:\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(bucks[(b, buck)]) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "Cell \u001b[1;32mIn[41], line 38\u001b[0m, in \u001b[0;36mLSHTs\u001b[1;34m(docs, words, B, R, NB, verbose)\u001b[0m\n\u001b[0;32m     36\u001b[0m perms \u001b[38;5;241m=\u001b[39m [np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mpermutation(M) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(P)]\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verbose: \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComputing buckets...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 38\u001b[0m buckets \u001b[38;5;241m=\u001b[39m \u001b[43mMakeBucketsT\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mperms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mNB\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m buckets\n",
      "\u001b[1;31mTypeError\u001b[0m: MakeBucketsT() missing 1 required positional argument: 'NB'"
     ]
    }
   ],
   "source": [
    "# Process the dataset\n",
    "words = set(word for doc in human_proteins_dataset for word in doc)\n",
    "word_index = make_word_indexes(words)\n",
    "indexed_docs = make_indexed_docs(human_proteins_dataset, word_index)\n",
    "indexed_docs = remove_empty_docs(indexed_docs)\n",
    "\n",
    "B = 20\n",
    "R = 5\n",
    "bucks = LSHTs(indexed_docs, len(words), B, R)\n",
    "\n",
    "for b, buck in bucks:\n",
    "    if len(bucks[(b, buck)]) > 1:\n",
    "        print(\"Band\", b, \"suggests these similar docs:\", bucks[(b, buck)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322a87b8-4102-43ff-ac7c-e748df7bcd1f",
   "metadata": {},
   "source": [
    "### Your short analysis here\n",
    "\n",
    "Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8043c37-3a4d-4c4d-ab3e-2fae63af06fa",
   "metadata": {},
   "source": [
    "## 3. Do dimensionality reduction \n",
    "\n",
    "Use the techniques discussued in class to make an appropriate dimensional reduction of the selected dataset. It is not necesary to be extensive, **it is better to select one approach and do it well than try a lot of techniques with poor insights and analysis**\n",
    "\n",
    "It is important to do some sensitivity analysis, relating the dataset size reduction to the loss of information\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdedab1a-54d8-49a3-8c08-4815bfc7d7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Add supporting functions here\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72095268-f6c8-4156-98ce-662952ba22fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Add processing code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547f4a00-01db-4c2c-a7b7-0a0f7ab0d333",
   "metadata": {},
   "source": [
    "## 4. Discuss your findings [to fill on your own]\n",
    "\n",
    "* Comment your results above\n",
    "* Discuss how could they be used for the full Uniprot that currently has about [248 Million proteins](https://www.uniprot.org/uniprotkb/statistics)\n",
    "\n",
    "\n",
    "Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
