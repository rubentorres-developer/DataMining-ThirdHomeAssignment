{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "333368e3-bb42-466c-8823-c45ed71f970f",
   "metadata": {},
   "source": [
    "# Prospecção de Dados (Data Mining) DI/FCUL - HA3\n",
    "\n",
    "## Third Home Assignement (MC/DI/FCUL - 2024)\n",
    "\n",
    "### Fill in the section below\n",
    "\n",
    "### GROUP:`02`\n",
    "\n",
    "* João Martins, 62532 - Hours worked on the project: 8\n",
    "* Rúben Torres, 62531 - Hours worked on the project: 8\n",
    "* Nuno Pereira, 56933 - Hours worked on the project: 8\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "The purpose of this Home Assignment is\n",
    "* Find similar items with Local Sensitivity Hashing\n",
    "* Do Dimensionality Reduction\n",
    "\n",
    "**NOTE 1: Students are not allowed to add more cells to the notebook**\n",
    "\n",
    "**NOTE 2: The notebook must be submited fully executed**\n",
    "\n",
    "**NOTE 3: Name of notebook should be: HA3_GROUP-XX.ipynb (where XX is the group number)**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a40f24-0da5-4b96-90f3-84259cbd1bc5",
   "metadata": {},
   "source": [
    "## 1. Read the Dataset\n",
    "\n",
    "The dataset correspond to about 99% of the Human Proteome (set of known Human Proteins - about 19,500), coded with specific structural elements. They are presented in a dictionary where the key is the [UniprotID](https://www.uniprot.org/) of the protein and the value is a set of indices of a specific structural characteristic\n",
    "\n",
    "Students can use one of two datasets, that are **not** subsets of each other: \n",
    "* `data_d3.pickle` - smaller set of structural features (2048)\n",
    "* `data_d4.pickle` - much larger set of structural features (20736) **Note:** This dataset has been Zipped to fit into moodle. Students should unzip it before usage \n",
    "\n",
    "Select **one** of the datasets and perform all analyses with it. \n",
    "\n",
    "It may be adviseable the usage of sparse matrices, especially for the `d4` dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa918e91-c325-4c1d-8e8d-93d411035a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your code Here\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import combinations\n",
    "\n",
    "#'A0A024R1R8', 'A0A024RBG1', 'A0A075B6H7', 'A0A075B6H8', 'A0A075B6H9', 'A0A075B6I0', 'A0A075B6I1'\n",
    "\n",
    "# human_proteins_dataset= pd.read_pickle(\"data_d3.pickle\")\n",
    "human_proteins_dataset: dict = pickle.load(open(\"data_d3.pickle\", \"rb\"))\n",
    "dataframe = pd.DataFrame(\n",
    "    list(human_proteins_dataset.items()), columns=[\"Id\", \"Proteome\"]\n",
    ")\n",
    "\n",
    "# print(human_proteins_dataset.keys())\n",
    "\n",
    "# for protein in dataframe[\"Proteome\"][:20]:\n",
    "#     print(protein)\n",
    "#     print(len(protein))\n",
    "\n",
    "all_protein = set(\n",
    "    protein for proteins in human_proteins_dataset.values() for protein in proteins\n",
    ")\n",
    "\n",
    "print(f\"all proteins: {all_protein}\")\n",
    "\n",
    "all_cods_protein = [key for key in human_proteins_dataset.keys()]\n",
    "\n",
    "print(all_cods_protein)\n",
    "\n",
    "group_protein_sets = [\n",
    "    set(human_proteins_dataset[protein]) for protein in human_proteins_dataset\n",
    "]\n",
    "\n",
    "print(f\"words {group_protein_sets[:2]}\")\n",
    "\n",
    "# pixa = dataframe[\"Proteome\"][15]\n",
    "# print(sorted(pixa))\n",
    "\n",
    "# dataframe[15:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853614f3-1e1b-4481-9784-e29dd0b453cb",
   "metadata": {},
   "source": [
    "## 2. Perform Local Sensitivity Hashing (LSH)\n",
    "\n",
    "* examine the selected dataset in terms of similarities and select a set of LSH parameters able to capture the most similar proteins\n",
    "* Comment your results\n",
    "\n",
    "**BONUS POINTS:** It might be interesting to identify **some** of the candidate pairs in Uniprot, to check if they share some of the same properties (e.g. for [protein P28223](https://www.uniprot.org/uniprotkb/P28223/entry))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead806d9-8073-4fc0-9668-68ad0b89077f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MakeBucketsT(TDocs, perms, N, B, R, NB):\n",
    "    Buckets = {}\n",
    "    all_docs = set(range(N))\n",
    "    for b in range(B):\n",
    "        SIGS = np.zeros((N, R), dtype=\"int32\")  # initializes line sig\n",
    "        for r in range(R):\n",
    "            perm = perms[b * R + r]\n",
    "            L = all_docs.copy()  # gets all docs as a set\n",
    "            i = 0\n",
    "            while len(L) > 0:\n",
    "                elem = perm[i]  # get new element  from permutation\n",
    "                docs_found = (\n",
    "                    TDocs[elem] & L\n",
    "                )  # get all the docs with a set bit on that elem that are still on the list\n",
    "                if len(docs_found) > 0:  # if anything was found\n",
    "                    SIGS[list(docs_found), r] = (\n",
    "                        i  #   set the line sig to the current position from the perm\n",
    "                    )\n",
    "                    L = (\n",
    "                        L - docs_found\n",
    "                    )  #   update the current list removing the found docs\n",
    "                i += 1  # update the current position\n",
    "                if i == 478:  # this is the case that the document is empty\n",
    "                    SIGS[list(L), r] = i  # Highly unlikely in a real data set\n",
    "                    L = {}\n",
    "                    # we have completed the signature for a given band,\n",
    "                    # now make the hashes for each document\n",
    "        for d in range(N):\n",
    "            bucket = hash(tuple(SIGS[d])) % NB\n",
    "            Buckets.setdefault((b, bucket), set()).add(d)\n",
    "    return Buckets\n",
    "\n",
    "\n",
    "def LSHTs(docs, words_len, B, R, NB=28934501, verbose=True):\n",
    "    N, M = len(docs), words_len\n",
    "    if verbose:\n",
    "        print(\"transpose the data set\")\n",
    "    data = [[] for i in range(M)]\n",
    "    for i, doc in enumerate(docs):\n",
    "        for word in doc:\n",
    "            data[word].append(i)\n",
    "    dataT = [set(L) for L in data]\n",
    "    P = B * R\n",
    "    np.random.seed(3)\n",
    "    if verbose:\n",
    "        print(\n",
    "            \"Generating %d permutations for %6.3f similarity\" % (P, (1 / B) ** (1 / R))\n",
    "        )\n",
    "    perms = [np.random.permutation(M) for i in range(P)]\n",
    "    if verbose:\n",
    "        print(\"Computing buckets...\")\n",
    "    buckets = MakeBucketsT(dataT, perms, N, B, R, NB)\n",
    "    return buckets\n",
    "\n",
    "\n",
    "def make_word_indexes(words) -> dict:\n",
    "    return dict(zip(words, range(len(words))))\n",
    "\n",
    "\n",
    "def make_indexed_docs(docs, word_index) -> list:\n",
    "    indexed_docs = []\n",
    "    for doc in docs:\n",
    "        indexed_doc = set(word_index[word] for word in doc)\n",
    "        indexed_docs.append(indexed_doc)\n",
    "    return indexed_docs\n",
    "\n",
    "\n",
    "def remove_empty_docs(words_text_sets) -> list:\n",
    "    return [doc for doc in words_text_sets if len(doc) > 0]\n",
    "\n",
    "\n",
    "def JaccardSimS(s1, s2):\n",
    "    return len(s1 & s2) / len(s1 | s2)\n",
    "\n",
    "\n",
    "def get_all_similar_docs(buckets):\n",
    "    sim_docs = []\n",
    "    for b, buck in buckets:\n",
    "        if len(buckets[(b, buck)]) > 1:\n",
    "            sim_docs += combinations(buckets[(b, buck)], 2)\n",
    "    sim_docs = set(sim_docs)\n",
    "    D = {}\n",
    "    for i, j in sim_docs:\n",
    "        D.setdefault(i, set()).add(j)\n",
    "        D.setdefault(j, set()).add(i)\n",
    "    return D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad80328c-ffba-4bb6-8ce3-03726ee82348",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the dataset\n",
    "\n",
    "protein_index = make_word_indexes(all_protein)\n",
    "group_protein_sets = remove_empty_docs(group_protein_sets)\n",
    "idx_protein = make_indexed_docs(group_protein_sets, protein_index)\n",
    "\n",
    "\n",
    "# print(f\"words {protein_index.keys()}\")\n",
    "# print(f\"word_index {group_protein_sets[:10]}\")\n",
    "# print(f\"indexed_docs {idx_protein[:10]}\")\n",
    "\n",
    "# print(sorted(idx_protein[9]))\n",
    "# print(sorted(idx_protein[15]))\n",
    "# print(JaccardSimS(idx_protein[9],idx_protein[15]))\n",
    "\n",
    "# print(all_cods_protein[17636])\n",
    "\n",
    "\n",
    "# common_elements = set(idx_protein[15]) == set(pixa)\n",
    "# print(\"Common elements:\", common_elements)\n",
    "\n",
    "BANDS = 40\n",
    "ROWS = 100\n",
    "bucks = LSHTs(idx_protein, len(protein_index), BANDS, ROWS)\n",
    "\n",
    "for b, buck in bucks:\n",
    "    if len(bucks[(b, buck)]) > 1:\n",
    "        print(\"Band\", b, \"suggests these similar docs:\", bucks[(b, buck)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478cdb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "######## MUST BE DELETED ########\n",
    "sim_docs = get_all_similar_docs(bucks)\n",
    "\n",
    "i = 27\n",
    "for j in sim_docs[i]:\n",
    "    print(\n",
    "        \"Documents %6d, and %6d, are %7.4f similar\"\n",
    "        % (i, j, JaccardSimS(idx_protein[i], idx_protein[j]))\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322a87b8-4102-43ff-ac7c-e748df7bcd1f",
   "metadata": {},
   "source": [
    "### Your short analysis here\n",
    "\n",
    "Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8043c37-3a4d-4c4d-ab3e-2fae63af06fa",
   "metadata": {},
   "source": [
    "## 3. Do dimensionality reduction \n",
    "\n",
    "Use the techniques discussued in class to make an appropriate dimensional reduction of the selected dataset. It is not necesary to be extensive, **it is better to select one approach and do it well than try a lot of techniques with poor insights and analysis**\n",
    "\n",
    "It is important to do some sensitivity analysis, relating the dataset size reduction to the loss of information\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdedab1a-54d8-49a3-8c08-4815bfc7d7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Add supporting functions here\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "from scipy.sparse import dok_array, csr_array, csc_array, bsr_array, lil_array\n",
    "\n",
    "\n",
    "def plot_component_representation(X, Y, xlabel, ylabel) -> None:\n",
    "    plt.plot(X, Y, label=\"SVD\")\n",
    "    plt.legend()\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72095268-f6c8-4156-98ce-662952ba22fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Add processing code here\n",
    "# Create a list of sets of proteins for each group\n",
    "group_protein_sets = [\n",
    "    set(human_proteins_dataset[protein]) for protein in human_proteins_dataset\n",
    "]\n",
    "\n",
    "print(group_protein_sets[:20])\n",
    "\n",
    "# Step 1: Find all unique proteins across all sets\n",
    "unique_proteins = sorted(\n",
    "    set(protein for proteins in group_protein_sets for protein in proteins)\n",
    ")\n",
    "\n",
    "print(unique_proteins)\n",
    "\n",
    "# Step 2: Create a mapping from proteins to column indices\n",
    "protein_to_index = {protein: idx for idx, protein in enumerate(unique_proteins)}\n",
    "\n",
    "print(protein_to_index)\n",
    "\n",
    "# Step 3: Initialize the binary matrix\n",
    "result_array = np.zeros((len(group_protein_sets), len(unique_proteins)), dtype=int)\n",
    "\n",
    "# Step 4: Populate the binary matrix\n",
    "for row_idx, proteins in enumerate(group_protein_sets):\n",
    "    for protein in proteins:\n",
    "        col_idx = protein_to_index[protein]\n",
    "        result_array[row_idx, col_idx] = 1\n",
    "\n",
    "# print(result_array[:20,:20])\n",
    "\n",
    "# # Step 5: Perform SVD\n",
    "u, s, v = np.linalg.svd(result_array) \n",
    "\n",
    "# Print the results with specified print options\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "\n",
    "print(\"U shape: \", u.shape)\n",
    "print(u)\n",
    "print(\"S shape: \", s.shape)\n",
    "print(s)\n",
    "print(\"V shape: \", v.shape)\n",
    "print(v)\n",
    "\n",
    "components_number = []\n",
    "consept_representation = []\n",
    "for i in range(len(s)):\n",
    "    # print(\"first %d components have a combined importance of %7.4f\" %(i+1, s[:i+1].sum()/s.sum()))\n",
    "    components_number.append(i)\n",
    "    consept_representation.append(s[: i + 1].sum() / s.sum())\n",
    "\n",
    "plot_component_representation(\n",
    "    components_number, \n",
    "    consept_representation,\n",
    "    \"Number of components\",\n",
    "    \"Combined importance of the previous components\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93df5d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result_array[:20,:20]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441e6f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "S_csc=csc_array(result_array)\n",
    "\n",
    "pca = TruncatedSVD(n_components=500)\n",
    "pca.fit_transform(S_csc)\n",
    "tve = 0\n",
    "for i, ve in enumerate(pca.explained_variance_ratio_):\n",
    "    tve += ve\n",
    "    print(\"PC%d - Variance explained: %7.4f - Total Variance: %7.4f\" % (i, ve, tve))\n",
    "print()\n",
    "# print(\"Actual Eigenvalues:\", pca.singular_values_)\n",
    "for i, comp in enumerate(pca.components_):\n",
    "    print(\"PC\", i, \"-->\", comp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547f4a00-01db-4c2c-a7b7-0a0f7ab0d333",
   "metadata": {},
   "source": [
    "## 4. Discuss your findings [to fill on your own]\n",
    "\n",
    "* Comment your results above\n",
    "* Discuss how could they be used for the full Uniprot that currently has about [248 Million proteins](https://www.uniprot.org/uniprotkb/statistics)\n",
    "\n",
    "\n",
    "Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
