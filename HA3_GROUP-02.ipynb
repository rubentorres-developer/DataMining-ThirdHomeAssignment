{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "333368e3-bb42-466c-8823-c45ed71f970f",
   "metadata": {},
   "source": [
    "# Prospecção de Dados (Data Mining) DI/FCUL - HA3\n",
    "\n",
    "## Third Home Assignement (MC/DI/FCUL - 2024)\n",
    "\n",
    "### Fill in the section below\n",
    "\n",
    "### GROUP:`02`\n",
    "\n",
    "* João Martins, 62532 - Hours worked on the project: 8\n",
    "* Rúben Torres, 62531 - Hours worked on the project: 8\n",
    "* Nuno Pereira, 56933 - Hours worked on the project: 8\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "The purpose of this Home Assignment is\n",
    "* Find similar items with Local Sensitivity Hashing\n",
    "* Do Dimensionality Reduction\n",
    "\n",
    "**NOTE 1: Students are not allowed to add more cells to the notebook**\n",
    "\n",
    "**NOTE 2: The notebook must be submited fully executed**\n",
    "\n",
    "**NOTE 3: Name of notebook should be: HA3_GROUP-XX.ipynb (where XX is the group number)**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a40f24-0da5-4b96-90f3-84259cbd1bc5",
   "metadata": {},
   "source": [
    "## 1. Read the Dataset\n",
    "\n",
    "The dataset correspond to about 99% of the Human Proteome (set of known Human Proteins - about 19,500), coded with specific structural elements. They are presented in a dictionary where the key is the [UniprotID](https://www.uniprot.org/) of the protein and the value is a set of indices of a specific structural characteristic\n",
    "\n",
    "Students can use one of two datasets, that are **not** subsets of each other: \n",
    "* `data_d3.pickle` - smaller set of structural features (2048)\n",
    "* `data_d4.pickle` - much larger set of structural features (20736) **Note:** This dataset has been Zipped to fit into moodle. Students should unzip it before usage \n",
    "\n",
    "Select **one** of the datasets and perform all analyses with it. \n",
    "\n",
    "It may be adviseable the usage of sparse matrices, especially for the `d4` dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa918e91-c325-4c1d-8e8d-93d411035a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "human_proteins_dataset = pickle.load(open(\"data_d3.pickle\", \"rb\"))\n",
    "\n",
    "proteomes = [proteome for proteome in human_proteins_dataset.keys()]\n",
    "\n",
    "group_protein_sets = [\n",
    "    set(human_proteins_dataset[protein]) for protein in human_proteins_dataset\n",
    "]\n",
    "\n",
    "unique_proteins = sorted(\n",
    "    set(protein for proteins in group_protein_sets for protein in proteins)\n",
    ")\n",
    "\n",
    "protein_to_index = {protein: index for index, protein in enumerate(unique_proteins)}\n",
    "\n",
    "result_array = np.zeros((len(group_protein_sets), len(unique_proteins)), dtype=int)\n",
    "for row_index, proteins in enumerate(group_protein_sets):\n",
    "    for protein in proteins:\n",
    "        col_index = protein_to_index[protein]\n",
    "        result_array[row_index, col_index] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853614f3-1e1b-4481-9784-e29dd0b453cb",
   "metadata": {},
   "source": [
    "## 2. Perform Local Sensitivity Hashing (LSH)\n",
    "\n",
    "* examine the selected dataset in terms of similarities and select a set of LSH parameters able to capture the most similar proteins\n",
    "* Comment your results\n",
    "\n",
    "**BONUS POINTS:** It might be interesting to identify **some** of the candidate pairs in Uniprot, to check if they share some of the same properties (e.g. for [protein P28223](https://www.uniprot.org/uniprotkb/P28223/entry))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead806d9-8073-4fc0-9668-68ad0b89077f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MakeBucketsTs(TDocs, perms, N, B, R, M, NB):\n",
    "    Buckets = {}\n",
    "    all_docs = set(range(N))\n",
    "    for b in range(B):\n",
    "        SIGS = np.zeros((N, R), dtype=\"int32\")\n",
    "        for r in range(R):\n",
    "            perm = perms[b * R + r]\n",
    "            L = all_docs.copy()\n",
    "            i = 0\n",
    "            while len(L) > 0:\n",
    "                elem = perm[i]\n",
    "                docs_found = TDocs[elem] & L\n",
    "                if len(docs_found) > 0:\n",
    "                    SIGS[list(docs_found), r] = i\n",
    "                    L = L - docs_found\n",
    "                i += 1\n",
    "                if i == M:\n",
    "                    SIGS[list(L), r] = i\n",
    "                    L = {}\n",
    "\n",
    "        for d in range(N):\n",
    "            bucket = hash(tuple(SIGS[d])) % NB\n",
    "            Buckets.setdefault((b, bucket), set()).add(d)\n",
    "    return Buckets\n",
    "\n",
    "\n",
    "def LSHT(Data, B, R, NB=28934501, verbose=True):\n",
    "    N, M = Data.shape\n",
    "    if verbose:\n",
    "        print(\"transpose the data set\")\n",
    "    DT = Data.T\n",
    "    DataT = [set(np.where(DT[i] == 1)[0]) for i in range(M)]\n",
    "    P = B * R\n",
    "    np.random.seed(3)\n",
    "    if verbose:\n",
    "        print(\n",
    "            \"Generating %d permutations for %6.3f similarity\" % (P, (1 / B) ** (1 / R))\n",
    "        )\n",
    "    perms = [np.random.permutation(M) for i in range(P)]\n",
    "    if verbose:\n",
    "        print(\"Computing buckets...\")\n",
    "    buckets = MakeBucketsTs(DataT, perms, N, B, R, M, NB)\n",
    "    return buckets\n",
    "\n",
    "\n",
    "def JaccardSim(d1, d2):\n",
    "    a = np.inner(d1, d2)\n",
    "    bc = np.sum(d1 + d2) - a\n",
    "    return a / bc\n",
    "\n",
    "\n",
    "def get_all_similar_docs(buckets):\n",
    "    sim_docs = []\n",
    "    for b, buck in buckets:\n",
    "        if len(buckets[(b, buck)]) > 1:\n",
    "            sim_docs += combinations(buckets[(b, buck)], 2)\n",
    "    sim_docs = set(sim_docs)\n",
    "    D = {}\n",
    "    for i, j in sim_docs:\n",
    "        D.setdefault(i, set()).add(j)\n",
    "        D.setdefault(j, set()).add(i)\n",
    "    return D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad80328c-ffba-4bb6-8ce3-03726ee82348",
   "metadata": {},
   "outputs": [],
   "source": [
    "BANDS = 80\n",
    "ROWS = 8\n",
    "\n",
    "similar = 0.6\n",
    "P = 1 - (1 - similar**ROWS) ** BANDS\n",
    "print(\n",
    "    \"The probability that documents share at least one band signature if they are %4.2f similar is: %7.4f\\n\"\n",
    "    % (similar, P)\n",
    ")\n",
    "\n",
    "similar = 0.70\n",
    "P = 1 - (1 - similar**ROWS) ** BANDS\n",
    "print(\n",
    "    \"The probability that documents share at least one band signature if they are %4.2f similar is: %7.4f\\n\"\n",
    "    % (similar, P)\n",
    ")\n",
    "\n",
    "bucks = LSHT(result_array, BANDS, ROWS)\n",
    "for b, buck in bucks:\n",
    "    if len(bucks[(b, buck)]) > 1:\n",
    "        print(\"Band\", b, \"suggests these similar docs:\", bucks[(b, buck)])\n",
    "        combs = combinations(bucks[(b, buck)], 2)\n",
    "        for i, j in combs:\n",
    "            jac = JaccardSim(result_array[i], result_array[j])\n",
    "            if jac >= 0.8:\n",
    "                print(\"Documents %6d and %6d are in truth %7.4f similar\" % (i, j, jac))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322a87b8-4102-43ff-ac7c-e748df7bcd1f",
   "metadata": {},
   "source": [
    "### Your short analysis here\n",
    "\n",
    "Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8043c37-3a4d-4c4d-ab3e-2fae63af06fa",
   "metadata": {},
   "source": [
    "## 3. Do dimensionality reduction \n",
    "\n",
    "Use the techniques discussued in class to make an appropriate dimensional reduction of the selected dataset. It is not necesary to be extensive, **it is better to select one approach and do it well than try a lot of techniques with poor insights and analysis**\n",
    "\n",
    "It is important to do some sensitivity analysis, relating the dataset size reduction to the loss of information\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdedab1a-54d8-49a3-8c08-4815bfc7d7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Add supporting functions here\n",
    "def plot_component_representation(X, Y, xlabel, ylabel) -> None:\n",
    "    plt.plot(X, Y, label=\"SVD\")\n",
    "    plt.legend()\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "def scatter_plot(concept_matrix, identifier, verbose = False) -> None:\n",
    "    plt.scatter(concept_matrix[:,0], concept_matrix[:,1], marker='o')\n",
    "    if verbose:\n",
    "        for i, txt in enumerate(identifier):\n",
    "            plt.annotate(txt, (concept_matrix[i,0], concept_matrix[i,1]))\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72095268-f6c8-4156-98ce-662952ba22fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Add processing code here\n",
    "u, s, v = np.linalg.svd(result_array) \n",
    "\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "\n",
    "print(\"U shape: \", u.shape)\n",
    "# print(u)\n",
    "print(\"S shape: \", s.shape)\n",
    "# print(s)\n",
    "print(\"V shape: \", v.shape)\n",
    "# print(v)\n",
    "\n",
    "components_number = []\n",
    "consept_representation = []\n",
    "for i in range(len(s)):\n",
    "    if round((s[:i+1].sum()/s.sum())*1000)/1000 == .9 :\n",
    "        print(\"first %d components have a combined importance of %7.4f\" %(i+1, s[:i+1].sum()/s.sum()))\n",
    "    components_number.append(i)\n",
    "    consept_representation.append(s[: i + 1].sum() / s.sum())\n",
    "\n",
    "plot_component_representation(\n",
    "    components_number, \n",
    "    consept_representation,\n",
    "    \"Number of singular values\",\n",
    "    \"Combined importance of the previous components\",\n",
    ")\n",
    "\n",
    "scatter_plot(u, proteomes)\n",
    "scatter_plot(v, proteins)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547f4a00-01db-4c2c-a7b7-0a0f7ab0d333",
   "metadata": {},
   "source": [
    "## 4. Discuss your findings [to fill on your own]\n",
    "\n",
    "* Comment your results above\n",
    "* Discuss how could they be used for the full Uniprot that currently has about [248 Million proteins](https://www.uniprot.org/uniprotkb/statistics)\n",
    "\n",
    "\n",
    "Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
